import scrapy
import json
import re
from coursespider.items import CourseItem
from scrapy_playwright.page import PageMethod
from urllib.parse import quote, urlparse

class BilibiliSpider(scrapy.Spider):
    name = "bilibili"
    allowed_domains = ["bilibili.com"]
    
    # æå–BVå·çš„æ­£åˆ™è¡¨è¾¾å¼
    BV_PATTERN = re.compile(r'BV[a-zA-Z0-9]{10}')

    def __init__(self, keywords=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if keywords:
            self.keywords = keywords.split(",")
        else:
            self.keywords = [
                "å¤§æ•°æ®", "æ•°æ®åº“", "æ•°æ®ç»“æ„", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "AIç®—æ³•",
                "åç«¯", "è½¯ä»¶å·¥ç¨‹", "ç½‘ç»œå®‰å…¨", "ç®—æ³•è¯¾ç¨‹", "ç¼–ç¨‹è¯­è¨€è¯¾ç¨‹", "è®¡ç®—æœºç½‘ç»œ", "è®¡ç®—æœºç³»ç»Ÿä½“ç³»ç»“æ„"
            ]

    def start_requests(self):
        for keyword in self.keywords:
            for page in range(1, 5):
                url = f"https://search.bilibili.com/all?keyword={quote(keyword)}&page={page}"
                yield scrapy.Request(
                    url,
                    headers={
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    },
                    meta={
                        "playwright": True,
                        "playwright_include_page": True,
                        "playwright_context": f"bilibili-{keyword}",
                        "playwright_page_methods": [
                            PageMethod("wait_for_selector", "div.bili-video-card__wrap", timeout=15000)
                        ],
                        "keyword": keyword
                    },
                    callback=self.parse,
                    errback=self.handle_error
                )

    def parse(self, response):
        keyword = response.meta.get("keyword")
        self.logger.info(f"ğŸ¯ æ­£åœ¨æŠ“å–å…³é”®è¯: {keyword}")

        courses = response.xpath('//div[contains(@class, "bili-video-card__wrap")]')
        self.logger.info(f"ğŸ“¦ è§†é¢‘æ•°é‡: {len(courses)}")

        for course in courses:
            title = ''.join(course.xpath('.//h3//text()').getall()).strip()
            href = course.xpath('.//a[@target="_blank"]/@href').get(default="").strip()
            full_url = response.urljoin(href) if href.startswith("/") else href

            if title and full_url:
                item = CourseItem()
                item["title"] = title
                item["url"] = full_url
                item["platform"] = "å“”å“©å“”å“©"
                item["learners"] = 0  # é»˜è®¤å€¼
                item["school"] = ""
                item["teacher"] = ""
                item["description"] = ""
                item["tags"] = keyword
                item["rating"] = None

                # æå–BVå·
                bv_match = self.BV_PATTERN.search(full_url)
                if bv_match:
                    bv_id = bv_match.group(0)
                    # ä½¿ç”¨APIè·å–æ”¶è—é‡
                    api_url = f"https://api.bilibili.com/x/web-interface/view?bvid={bv_id}"
                    yield scrapy.Request(
                        api_url,
                        callback=self.parse_api,
                        meta={"item": item},
                        headers={
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
                            "Referer": full_url
                        },
                        errback=self.handle_api_error
                    )
                else:
                    self.logger.warning(f"âš ï¸ æ— æ³•æå–BVå·: {full_url}")
                    yield item

    def parse_api(self, response):
        """è§£æAPIè¿”å›çš„æ•°æ®"""
        item = response.meta["item"]
        try:
            data = json.loads(response.text)
            if data.get("code") == 0:
                # æå–æ”¶è—é‡
                fav_count = data["data"]["stat"]["favorite"]
                item["learners"] = fav_count
            else:
                self.logger.warning(f"APIè¯·æ±‚å¤±è´¥: {response.url}, é”™è¯¯ç : {data.get('code')}")
        except Exception as e:
            self.logger.error(f"è§£æAPIå“åº”å¤±è´¥: {e}")
        
        yield item

    def handle_api_error(self, failure):
        """å¤„ç†APIè¯·æ±‚é”™è¯¯"""
        item = failure.request.meta["item"]
        self.logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥: {failure.request.url}ï¼ŒåŸå› : {repr(failure.value)}")
        yield item

    def handle_error(self, failure):
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.request.url}ï¼ŒåŸå› : {repr(failure.value)}")